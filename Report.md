# Abstract (1000)
### Ideas, bits and parts

How do you differenciate a cell from a simple circle ? Answering such question sounds trivial at first glance: you can tell visually and from your experience. However, when trying to implement such decision in a computer program the question complexifies. 

With the increasing amount of data generated by biology experiment

Classification and segmentation is a good example for machine learning usezfullness. How do you differenciate a cell from a simple round and how do you scale up your answer without increasing exponentially the time of analysis ?

### Abstract v1
Computer sciences has become a pivotal discipline when approaching biological problems. Indeed, a majority of questions in biology requires the use of big and complex datasets. Hence, the ability to treat these data is crucial. Machine learning provides in that sense a powerfull tool for such problems. Machine learning relies on the principle that a program can "learn" to recognize specific patterns and can thus predict an output from an input. Such tool is extremely useful in image analysis as it enables a large scale and fast analysis. Indeed a major problem in image analysis is segmentation, the ability to distinguish between to separate region. In this report we report a method for quantifying the ability of machine learnng to perform segmentation on images. We discuss different metrics that can be use to supervise such process and we propose a method to compare different network architectures and outputs.

**Key-words:** Machine learning, segmentation, quantification, ...

### Abstract v2

Computer sciences has become a pivotal discipline when approaching biological problems. In microscopy for instance, the instruments and the growing scalability of the experiments has lead the data analysis process to be computer dependent. In the recent years machine learning approaches have been proven to be a powerfull tool for analysis. Machine learning relies on the principle that a program can "learn" to recognize specific patterns and can thus predict an output from an input. A classical problem in image analysis is segmentation, how do you set rules for determining boundaries in data. Fortunatly, such problem is commonly encountered in computer sciences and many techniques have been developped. In this report we report a method for quantifying the ability of machine learnng to perform segmentation on images. We discuss different metrics that can be use to supervise such process and we propose a method to compare different network architectures and outputs.

**Key-words:** Machine learning, segmentation, quantification, ...

### Abstract v3 

#### Evaluating machine learning performance : development of quantification protocol to evaluate AI segmentation algorithms

A.Kanso $*$ , N.Louafi  , M.Nollman**

$*$ Corresponding author: [kanso.ali@outlook.fr](mailto:kanso.ali@outlook.fr) 

** Project Supervisor: [marcelo.nollmann@cbs.cnrs.fr](mailto:marcelo.nollmann@cbs.cnrs.fr) 

Computer science has become a pivotal discipline when approaching biological problems. In microscopy for instance, the instruments and the growing scalability of the experiments have led the data analysis process to be computer dependent. Machine learning, as it has been proven recently to be a powerful tool for analysis, relies on the concept that a program can learn from data, and thus recognize specific patterns and make decisions to predict an output. When talking about image analysis, the fact that we must set rules for determining boundaries in data, makes image segmentation a major problem. Fortunately, such problem is commonly encountered in computer sciences and many techniques have been developed. In this report we introduce a method that quantifies the "accuracy" of machine learning in performing image segmentation and we discuss different metrics that can be used to supervise such process. We also propose a method to compare different network architectures and outputs.


# Introduction (5000 characters)
**Topics to introduce**

- How computer sciences help biology
- Introduce the concept of neural networks
- Segmentation 
- How there are no real way of quantifying a network performance
- We used both qualitative (gallery) and quantitative approaches
- Figure that represents the workflow of the method developed
- Why is it important to automatize the segmentation (see Lab1 description)
- What the images will be used for ? 
- Introduce the problematic of we suspect a tendency to miss bright objects 

[[figures#Figure 1]] principle of a neural network 


# Methods (2500 characters)

**count : 2832 characters...** si j'enlève la partie environment ca fait 2529

( *Setting up an environment:* 
We set up a linux based computer for the analysis. The actual calculations were performed on a remote server via *ssh*. We used jupyter notebook to write the scripts and used an anaconda environment to use all the required python packages. )

*Neural network:*
The networks were based on Stardist (citation). This algorithm is based on the prediction of star-convex polygons to segment images. More specifically we worked with three different network based on different trainings. The first, that we will name *simulated network* was trained using simulated images (gaussian-shape objects). The second one named *data network* was trained using experimental data labeled by hand. The final one named *retrained network* is based on the simulated network but with an addition of images that will be develop in the following section. 

*Qualitative network performance:* 
To evaluate qualitatively the network performance we used the package scikit image (citation) to retrieve the coordinates of the different labeled objects. We then transposed those coordinates into the raw image and extracted a 10x10 region of the image. This porcess was done iteratively on all the segmented object and a gallery was then created. Furthermore the gallery was classified by increasing intensity (from left to right) by comparing the maximum fluorescence of every snippets of the raw image. To make sure this measure was accurate for all the objects and correcting for out of focus objects the extraction was done at the *z-center position* of the labeled object. 

*Quantitative exploration of the results:*
Using the labeled image, we gathered the statistics of every detected objects using the package scikit image and calculated the following properties: volume, maximum intensity, the ID of the detected object. 

*Intensity sensitivity analysis:* 
To assess the network tendency to fail the detection of bright objects a comparison analysis was developed using the algorithm Astropy as a standard (citation). This algorithm is based on intensity thresholding to detect objects. To compared we calculated an *accuracy* metric described as the ratio between the number of object detected by both method divided by the number of object detected by the Astropy algorithm. 

*Correction of a network:*
Segmented images were submitted to Astropy analysis. For every Astropy detection the coordinate were reported on the segmented image if at this location (x,y,z) we verify if the pixel value was superior to 0 in which case meant a segmented object. In the other situation we classified the objects as missed and cropped  (256,256,70) images in the labeled image and the raw image for the training. This process was done on n=4 (2048,2048,70) images and the corrected images were split between a training and a testing dataset. 


# Results (3000 characters)

**(3021 characters ... )**

### Qualitative approach to network performance

[[figures#Figure 2]] first gallery acquired 

**(772 characters)**

Our first approach to the question of evaluating segmentation performance was to qualitatively assess the segmentation. The network outputs stacks of 2048x2048x70  which are complicated to visually inspect. To visualize the objects we thus relied on the gallery of images described in the material section. This gallery represents every objects the network considered as "nuclei". Although not quantitative, this step allowed us to have a first idea on the kind of objects outputted. An example of output is present in figure 1. We see that the network segmented a majority of bright and round shaped objects which was expected. Moreover, some objects were close to background level. However, we see that some objects are **noise** which indicates a poor network performance. 

### Quantitative approach to network performance

**(517 characters)**

Having a qualitative approach allowed us to have a first insight to a network performance. However, we wanted to have a quantitative analysis to be able to compare performance between network and also between trainings. As described in the method we used various properties of the objects to compare networks. Figure 3 shows the comparison between two different networks. We see that there is a clear difference in network performance as one network has a tendency to segment bigger objects and with higher intensity.

[[figures#Figure 3]] statistics that compares 2 networks


### Further study on intensity sensitivity

**(970 characters)**

To study the dependency to intensity of the segmentation we decided to compare our neural network approach to an intensity based segmentation. The Astropy algorithm (inserer ref) was originally developed to segment images of the sky to find bright objects: stars. To qualitatively evaluate such comparison we relied on the *accuracy* metric described in the method section. This metric will yield higher value for images where all the high intensity objects were detected by the neural network. As shown in figure X not all the bright objects were segmented by the network. This conclusion confirmed earlier visual observation that bright objects tends to be "missed". Considering the importance of bright fluorescent events we focused on finding a way to correct for missed events. To do so, we decided to **play on** the training of the network. The neural network is trained using a set of images and tries to generalize to be able to be used on a variety of images. 

figure : figure showing Astropy vs stardist and the fact that the networks misses objects ? 
[[figures#Figure 4]]

**(391 characters)**

The network was given 500 epochs of 100 steps. Statistics of the training can be found in annex X. Classically one can look at the contingency matrix but that is assuming a ground truth is known. To evaluate the quality of the training and thus the performance of the network we performed a segmentation with the new network on a never-seen image that yielded the results shown in figure XX.

[[figures#Figure 8]] , [[figures#Figure 6]] statistics of the retraining and the gallery

**(317 characters)**

We compared the results with the first network we worked with, described above. We observe that as the object detected seem coherent (panel A) we observe a loss of the low intensity objects . Moreover, the amount of detection has decreased as well as the volume distribution. These results were not the expected ones.  

### Correcting a network 
Introduce the concept of network training, trainin vs testing dataset , ground truth ?
Introduce the concept of the correction :
- Compare Astropy stardist 
- if not the same add a template sphere in the image
- why did we chose this template sphere ?
- cut the image
- relabel the image
- save only the corrected subimages
### Retraining effects

Discuss the improvement of the training if there was 

Probably not all the plots but maybe a combination if the concepts were introduced before ? 

# Discussion (1500)


Figure X:  General workflow of the lab1

- discuss the fact that low intensity object were not corrected for ?
- optimizatyion of the code to work faster 
- optimization of thye code to work on bigger image without surcharging the GPU 
- necessity for such analysis , relevance of needing a corrction for 20% of the opbjects ? 

# Annex

[[figures#Figure 9]]
training of the network

[[figures#Figure 5]]
general workflow of the lab1

table to summarize which script does what 


# References (max 20)

**count : 7**

**Stardist :**
Schmidt, U., Weigert, M., Broaddus, C., & Myers, G. (2018). Cell Detection with Star-Convex Polygons. In A. F. Frangi, J. A. Schnabel, C. Davatzikos, C. Alberola-López, & G. Fichtinger (Éds.), _Medical Image Computing and Computer Assisted Intervention – MICCAI 2018_ (Vol. 11071, p. 265‑273). Springer International Publishing. [https://doi.org/10.1007/978-3-030-00934-2_30](https://doi.org/10.1007/978-3-030-00934-2_30)

**Machine learning reviews :**

Greener, J. G., Kandathil, S. M., Moffat, L., & Jones, D. T. (2022). A guide to machine learning for biologists. _Nature Reviews Molecular Cell Biology_, _23_(1), 40‑55. [https://doi.org/10.1038/s41580-021-00407-0](https://doi.org/10.1038/s41580-021-00407-0)

Moen, E., Bannon, D., Kudo, T., Graf, W., Covert, M., & Van Valen, D. (2019). Deep learning for cellular image analysis. _Nature Methods_, _16_(12), 1233‑1246. [https://doi.org/10.1038/s41592-019-0403-1](https://doi.org/10.1038/s41592-019-0403-1)

von Chamier, L., Laine, R. F., & Henriques, R. (2019). Artificial intelligence for microscopy : What you should know. _Biochemical Society Transactions_, _47_(4), 1029‑1040. [https://doi.org/10.1042/BST20180391](https://doi.org/10.1042/BST20180391)

**pyHiM:**

Cardozo Gizzi, A. M., Cattoni, D. I., Fiche, J.-B., Espinola, S. M., Gurgo, J., Messina, O., Houbron, C., Ogiyama, Y., Papadopoulos, G. L., Cavalli, G., Lagha, M., & Nollmann, M. (2019). Microscopy-Based Chromosome Conformation Capture Enables Simultaneous Visualization of Genome Organization and Transcription in Intact Organisms. _Molecular Cell_, _74_(1), 212-222.e5. [https://doi.org/10.1016/j.molcel.2019.01.011](https://doi.org/10.1016/j.molcel.2019.01.011)

Espinola, S. M., Götz, M., Bellec, M., Messina, O., Fiche, J.-B., Houbron, C., Dejean, M., Reim, I., Cardozo Gizzi, A. M., Lagha, M., & Nollmann, M. (2021). Cis-regulatory chromatin loops arise before TADs and gene activation, and are independent of cell fate during early Drosophila development. _Nature Genetics_, _53_(4), 477‑486. [https://doi.org/10.1038/s41588-021-00816-z](https://doi.org/10.1038/s41588-021-00816-z)
**Astropy**

Larry Bradley, Brigitta Sipőcz, Thomas Robitaille, Erik Tollerud, Zé Vinícius, Christoph Deil, Kyle Barbary, Tom J Wilson, Ivo Busko, Axel Donath, Hans Moritz Günther, Mihai Cara, P. L. Lim, Sebastian Meßlinger, Simon Conseil, Azalee Bostroem, Michael Droettboom, E. M. Bray, Lars Andersen Bratholm, … Harrison Souchereau. (2022). astropy/photutils: (1.4.0). Zenodo. [https://doi.org/10.5281/zenodo.6385735](https://doi.org/10.5281/zenodo.6385735)


